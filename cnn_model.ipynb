{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "# remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the JSON file and handle multiple JSON objects\n",
    "def load_json_data(json_path, limit=10000):\n",
    "    data = []\n",
    "    with open(json_path, 'r') as f:\n",
    "        for line in f:  # Read file line by line in case it's not an array\n",
    "            try:\n",
    "                data.append(json.loads(line))  # Load each JSON object line by line\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e}\")\n",
    "    \n",
    "    # Shuffle the data and take a subset of size 'limit'\n",
    "    random.shuffle(data)\n",
    "    data = data[:limit]  # Select only the first 'limit' samples\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = 'photos.json'  # Update with the correct path\n",
    "json_data = load_json_data(json_file_path, limit=10000)  # Limit to 10,000 samples\n",
    "# Step 2: Add Preprocessing Functions (Detailed Preprocessing)\n",
    "\n",
    "def apply_threshold(image, threshold_value=127):\n",
    "    if len(image.shape) == 3 and image.shape[2] == 3:\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray_image = image\n",
    "    \n",
    "    _, thresh_image = cv2.threshold(gray_image, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "    return thresh_image\n",
    "def apply_histogram_equalization(image):\n",
    "    if len(image.shape) == 3 and image.shape[2] == 3:\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray_image = image\n",
    "    \n",
    "    equalized_image = cv2.equalizeHist(gray_image)\n",
    "    return equalized_image\n",
    "def apply_gaussian_blur(image, kernel_size=(5, 5)):\n",
    "    blurred_image = cv2.GaussianBlur(image, kernel_size, 0)\n",
    "    return blurred_image\n",
    "def detailed_preprocessing(image):\n",
    "    thresholded_image = apply_threshold(image)\n",
    "    equalized_image = apply_histogram_equalization(thresholded_image)\n",
    "    blurred_image = apply_gaussian_blur(equalized_image)\n",
    "    return blurred_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Load Images from JSON metadata and apply preprocessing\n",
    "\n",
    "def load_images_with_preprocessing(image_dir, json_data, img_size=(224, 224)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for item in json_data:\n",
    "        photo_id = item['photo_id']\n",
    "        label = item['label']\n",
    "        \n",
    "        image_path = os.path.join(image_dir, f\"{photo_id}.jpg\")\n",
    "        if os.path.exists(image_path):\n",
    "            image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "            \n",
    "            if image is None:\n",
    "                print(f\"Warning: Failed to load image {image_path}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                image = cv2.resize(image, img_size)\n",
    "            except Exception as e:\n",
    "                print(f\"Error resizing image {image_path}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            image = detailed_preprocessing(image)\n",
    "            image = image.astype('float32') / 255.0\n",
    "            \n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "        else:\n",
    "            print(f\"Image {image_path} not found.\")\n",
    "    \n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    if images.ndim == 3:\n",
    "        images = np.expand_dims(images, axis=-1)\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the photos are stored\n",
    "image_dir = './photos'\n",
    "\n",
    "# Load images and labels from the JSON data\n",
    "images, labels = load_images_with_preprocessing(image_dir, json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Convert Labels to Numerical Values using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(labels)\n",
    "# Step 5: Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "if X_train.shape[-1] == 1:\n",
    "    X_train = np.repeat(X_train, 3, axis=-1)\n",
    "    X_test = np.repeat(X_test, 3, axis=-1)\n",
    "# Step 6: Data Augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "test_datagen = ImageDataGenerator()\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=32)\n",
    "test_generator = test_datagen.flow(X_test, y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Build CNN Model\n",
    "def build_cnn_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "# Compute class weights\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                  classes=np.unique(y_train),\n",
    "                                                  y=y_train)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "print(f\"Class Weights: {class_weight_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Train and Evaluate Final Model\n",
    "\n",
    "final_model = build_cnn_model()\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model with class weights\n",
    "final_model.fit(train_generator,\n",
    "                validation_data=test_generator,\n",
    "                epochs=3,\n",
    "                callbacks=[early_stopping],\n",
    "                class_weight=class_weight_dict)  # Pass class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Evaluate the Model\n",
    "\n",
    "# Evaluate on test data\n",
    "test_loss, test_acc = final_model.evaluate(test_generator)\n",
    "print(f\"Final Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "old_acc = test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Check the number of unique classes\n",
    "num_classes = len(np.unique(y_test))\n",
    "print(f\"Number of classes in dataset: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Ensure y_test is one-hot encoded for AUC calculation\n",
    "y_test_one_hot = to_categorical(y_test, num_classes=num_classes)\n",
    "# Step 3: Predict test set probabilities and labels\n",
    "y_pred_prob = final_model.predict(X_test)  # Probabilities for each class\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)    # Get the predicted class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check shapes\n",
    "print(f\"Shape of y_test_one_hot: {y_test_one_hot.shape}\")\n",
    "print(f\"Shape of y_pred_prob: {y_pred_prob.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: AUC-ROC Curve for each class\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "roc_auc = {}\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for i in range(num_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_one_hot[:, i], y_pred_prob[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    plt.plot(fpr[i], tpr[i], label=f'Class {i} (area = {roc_auc[i]:.2f})')\n",
    "\n",
    "# Plotting AUC-ROC Curve\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)  # Dashed diagonal line\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('AUC-ROC Curve for Multi-Class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot Confusion Matrix using Seaborn heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n",
    "old_loss,old_acc = final_model.evaluate(X_test, y_test, verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Hyperperameter Tuning\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001)\n",
    "\n",
    "# Re-train the model with ReduceLROnPlateau\n",
    "final_model.fit(train_generator,validation_data=test_generator,epochs=3,callbacks=[early_stopping, reduce_lr],class_weight=class_weight_dict)\n",
    "# Increase the number of epochs and Added ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=3, shuffle=True, random_state=42) \n",
    "\n",
    "fold_no = 1\n",
    "for train_index, val_index in kf.split(X_train):    \n",
    "    print(f'Training fold no.{fold_no}...')         \n",
    "    X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]    \n",
    "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]       \n",
    "    # Train the model on the current fold\n",
    "    final_model = build_cnn_model() \n",
    "    # Re-initialize model for each fold\n",
    "    final_model.fit(X_train_fold, y_train_fold, epochs=3, validation_data=(X_val_fold, y_val_fold), class_weight=class_weight_dict)\n",
    "    # Evaluate the model on the validation set of the current fold\n",
    "    val_loss, val_acc = final_model.evaluate(X_val_fold, y_val_fold)\n",
    "    print(f'Fold{fold_no}- Validation Accuracy:{val_acc * 100:.2f}%') \n",
    "    fold_no += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = final_model.fit(train_generator,\n",
    "                          validation_data=test_generator,\n",
    "                          epochs=3,  # Set your desired number of epochs\n",
    "                          callbacks=[early_stopping, reduce_lr],\n",
    "                          class_weight=class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training vs validation accuracy and loss curves\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training vs Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'],label='Training Loss')\n",
    "plt.plot(history.history['val_loss'],label='Validation Loss')\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_loss,new_acc = final_model.evaluate(X_test, y_test, verbose=1) \n",
    "\n",
    "print(\"Model Accuracy before Tuning\",test_acc,\"Model Tuning After Tuning\",new_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict random images from the dataset and show results\n",
    "def predict_random_images(generator, model, label_encoder, num_images=4):\n",
    "    # Get a batch of images and true labels\n",
    "    images, true_labels = next(generator)  # Get one batch of images from the generator\n",
    "    \n",
    "    # Predict using the trained model\n",
    "    predictions = model.predict(images)\n",
    "\n",
    "    # Convert predicted probabilities to class indices\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Convert true labels to class indices if they are one-hot encoded\n",
    "    if true_labels.ndim == 2:  # for one-hot encoding\n",
    "        true_labels = np.argmax(true_labels, axis=1)\n",
    "    \n",
    "    # Get the class labels using the label_encoder\n",
    "    class_labels = label_encoder.inverse_transform(range(len(label_encoder.classes_)))\n",
    "\n",
    "    # Plot random images with true labels and predictions\n",
    "    plt.figure(figsize=(8,8))\n",
    "    for i in range(num_images):\n",
    "        random_index = random.randint(0, len(images) - 1)  # Pick a random image from the batch\n",
    "        plt.subplot(2, num_images // 2, i + 1)\n",
    "        plt.imshow(images[random_index])\n",
    "        plt.title(f\"True: {class_labels[true_labels[random_index]]}\\nPred: {class_labels[predicted_labels[random_index]]}\")\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "predict_random_images(train_generator, final_model, label_encoder, num_images=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
